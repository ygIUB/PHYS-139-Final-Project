{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8324cb18-a848-423b-a9b1-cccbd58b3134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/sabandyopadhyay/.local/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/sabandyopadhyay/.local/lib/python3.11/site-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZmZ1RG796ClDXdjM_TKP6RGAd-pNKZfH&confirm=t\n",
      "To: /home/sabandyopadhyay/phys139_239/PHYS-139-Final-Project/cem_mitolab.zip\n",
      " 81%|██████████████████████████████▋       | 2.38G/2.96G [00:39<00:07, 77.7MB/s]Error:\n",
      "\n",
      "\t[Errno 122] Disk quota exceeded\n",
      "\n",
      "To report issues, please visit https://github.com/wkentaro/gdown/issues.\n",
      " 81%|██████████████████████████████▋       | 2.39G/2.96G [00:40<00:09, 59.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!python3 -m gdown \"1ZmZ1RG796ClDXdjM_TKP6RGAd-pNKZfH&confirm=t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16989cc2-265a-4b52-8143-d74263ea6f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/HP ENVY/Desktop/Phys 139/cem_mitolab/cem_mitolab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/HP ENVY/Desktop/Phys 139/cem_mitolab/cem_mitolab\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# parent folder that contains MANY dataset folders\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiFolderSegmentationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# mitochondria = foreground/background\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mMultiFolderSegmentationDataset.__init__\u001b[0;34m(self, root, size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 27\u001b[0m dataset_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_dataset_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m dataset_dirs:\n\u001b[1;32m     30\u001b[0m     images_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m, in \u001b[0;36mfind_dataset_dirs\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_dataset_dirs\u001b[39m(root):\n\u001b[1;32m     12\u001b[0m     dataset_dirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     14\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/HP ENVY/Desktop/Phys 139/cem_mitolab/cem_mitolab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def find_dataset_dirs(root):\n",
    "    dataset_dirs = []\n",
    "    for name in sorted(os.listdir(root)):\n",
    "        path = os.path.join(root, name)\n",
    "        if os.path.isdir(path):\n",
    "            if (os.path.isdir(os.path.join(path, \"images\")) and\n",
    "                os.path.isdir(os.path.join(path, \"masks\"))):\n",
    "                dataset_dirs.append(path)\n",
    "    return dataset_dirs\n",
    "\n",
    "class MultiFolderSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, size=(256,256)):\n",
    "        self.size = size\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        dataset_dirs = find_dataset_dirs(root)\n",
    "\n",
    "        for ds in dataset_dirs:\n",
    "            images_dir = os.path.join(ds, \"images\")\n",
    "            masks_dir = os.path.join(ds, \"masks\")\n",
    "\n",
    "            img_files = sorted(os.listdir(images_dir))\n",
    "\n",
    "            for f in img_files:\n",
    "                img_path = os.path.join(images_dir, f)\n",
    "                mask_path = os.path.join(masks_dir, f)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.images.append(img_path)\n",
    "                    self.masks.append(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.images[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(self.masks[idx], 0)\n",
    "\n",
    "        # resize\n",
    "        img = cv2.resize(img, self.size, interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, self.size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # normalize + convert to tensor\n",
    "        img = img / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW\n",
    "\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.down1 = DoubleConv(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.conv4 = DoubleConv(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv2 = DoubleConv(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv1 = DoubleConv(128, 64)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.down1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "\n",
    "        c2 = self.down2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "\n",
    "        c3 = self.down3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "\n",
    "        c4 = self.down4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        up4 = self.up4(bn)\n",
    "        merge4 = torch.cat([up4, c4], dim=1)\n",
    "        c5 = self.conv4(merge4)\n",
    "\n",
    "        up3 = self.up3(c5)\n",
    "        merge3 = torch.cat([up3, c3], dim=1)\n",
    "        c6 = self.conv3(merge3)\n",
    "\n",
    "        up2 = self.up2(c6)\n",
    "        merge2 = torch.cat([up2, c2], dim=1)\n",
    "        c7 = self.conv2(merge2)\n",
    "\n",
    "        up1 = self.up1(c7)\n",
    "        merge1 = torch.cat([up1, c1], dim=1)\n",
    "        c8 = self.conv1(merge1)\n",
    "\n",
    "        return self.out_conv(c8)\n",
    "\n",
    "def train(model, dataloader, device, epochs=10):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for imgs, masks in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "root_dir = \"C:/Users/HP ENVY/Desktop/Phys 139/cem_mitolab/cem_mitolab\"   # parent folder that contains MANY dataset folders\n",
    "dataset = MultiFolderSegmentationDataset(root_dir)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = UNet(n_classes=2)   # mitochondria = foreground/background\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train(model, loader, device, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c51c8-e897-451d-93f5-5af0cf4264b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
